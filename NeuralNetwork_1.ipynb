{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc1f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b1aec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:07<00:00, 1.31MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 70.5kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:03<00:00, 483kB/s] \n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 13.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "# data loader for MNIST\n",
    "# defines a transformation that converts PIL images into pytorch tensors between 0 and 1\n",
    "transform = transforms.ToTensor()\n",
    "# downloading MNIST training and test datasets\n",
    "train_set = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "# DataLoader wraps the datasets to provide batching, shuffling and parallel loading \n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # fc1 -> fully connected layer transforming 784 inputs (flattened 28x28 image) to 128 neurons.\n",
    "    # relu1 -> ReLU activation introduces non-linearity\n",
    "    # fc2 -> fully connected layer maps 128 neurons to 64\n",
    "    # fc2 -> outputs 10 logits (0-9)\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 10) # 10 output classes\n",
    "    \n",
    "    # x.view(-1, 28*28) reshapes the 2D image to 1D tensor (784 features1)\n",
    "    # -1 lets pytorch infer the batch size\n",
    "    # output is raw scores (logits) not probabilities \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28) # flatten\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.fc3(x)    # raw(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be026bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.1023\n"
     ]
    }
   ],
   "source": [
    "# initialize network, loss, optimizer\n",
    "model = MLP()\n",
    "# defines teh loss function -> CE combines softmax and negative log likelihood for classification \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# adam optimiser\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# train the network (1 epoch for brevity)\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    for xb, yb in train_loader:\n",
    "        # clears gradients from previous step \n",
    "        optimizer.zero_grad()\n",
    "        # runs forward pass on input batch xb, outputs logits\n",
    "        out = model(xb)\n",
    "        # calculates the loss\n",
    "        loss = criterion(out, yb)\n",
    "        # computes gradients via backpropogation\n",
    "        loss.backward()\n",
    "        # updates weights using gradients \n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98871987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.20%\n"
     ]
    }
   ],
   "source": [
    "# evaluate accuracy \n",
    "# sets the model to evaluation mode (disables output / batch norm updates)\n",
    "model.eval()\n",
    "correct = 0 # counts number of correct predictions \n",
    "total = 0   # sums total samples \n",
    "\n",
    "# disables gradient calculations for faster inference and lower memory\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        preds = model(xb).argmax(dim=1) # gets predicted class indices from logits\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "print(f'Test Accuracy: {correct / total:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d442982",
   "metadata": {},
   "source": [
    "### Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9a08a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "# transform = transforms.ToTensor()\n",
    "# train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9719f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_to_numpy(dataset):\n",
    "    \"\"\"\n",
    "    Convert torch dataset to a tuple of (X,y) numpy arrays,\n",
    "    flattening images to 1D vectors\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for img, label in dataset:\n",
    "        X.append(img.numpy().flatten()) # flatten 28x28 to 784\n",
    "        Y.append(label)\n",
    "    return np.stack(X), np.array(Y)\n",
    "\n",
    "X_train, y_train = torch_to_numpy(train_set)\n",
    "X_test, y_test = torch_to_numpy(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0343a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the labels for use with softmax/CE\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    convert vector of numeric class labels to one-hot encoded matrix.\n",
    "    [1,4] => [[0 1 0 0 0 0 0 0 0 0], [0 0 0 0 1 0 0 0 0 0]]\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[y]\n",
    "y_train_oh = one_hot(y_train)\n",
    "y_test_oh = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1214222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize neural network parameters\n",
    "input_size = 784    # MNIST images are 28*28 pixels\n",
    "hidden_size = 128   # number of neorons in hidden layer\n",
    "output_size = 10    # output classes (0-9)\n",
    "\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01 # weights for input->hidden\n",
    "b1 = np.zeros((1, hidden_size)) # bias for hidden\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01 # weights for hidden->output\n",
    "b2 = np.zeros((1, output_size)) # bias for output\n",
    "\n",
    "# define activation functions and loss\n",
    "def relu(x):\n",
    "    \"\"\"Apply the ReLU (Rectified Linear Unit) function elementwise\"\"\"\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    \"\"\"Derivative of ReLU: 1 if x > 0, else 0\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Apply softmax to each row of x for classification.\n",
    "    Computes normalized probabilities for each class\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True)) # stability\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute the mean CE loss (for softmax one-hot)\n",
    "    y_pred: predicted probabilities; y_true: one-hot true labels.\n",
    "    \"\"\"\n",
    "    eps = 1e-9\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred + eps), axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36003cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 0: loss = 2.3037\n",
      "Epoch 1, batch 100: loss = 0.9558\n",
      "Epoch 1, batch 200: loss = 0.5121\n",
      "Epoch 1, batch 300: loss = 0.2371\n",
      "Epoch 1, batch 400: loss = 0.3362\n",
      "Epoch 1, batch 500: loss = 0.1789\n",
      "Epoch 1, batch 600: loss = 0.4208\n",
      "Epoch 1, batch 700: loss = 0.2471\n",
      "Epoch 1, batch 800: loss = 0.4152\n",
      "Epoch 1, batch 900: loss = 0.1429\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "lr = 0.1\n",
    "batch_size = 64 # small batches for stochastic gradient descent-like behaviour \n",
    "epochs = 1\n",
    "\n",
    "N = X_train.shape[0]\n",
    "for epoch in range(epochs):\n",
    "    perm = np.random.permutation(N) # shuffle the dataset each epoch\n",
    "    X_train, y_train_oh, = X_train[perm], y_train_oh[perm]\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        # Select batch \n",
    "        xb = X_train[i:i+batch_size]\n",
    "        yb = y_train_oh[i:i+batch_size]\n",
    "\n",
    "        # Forward pass (compute scores and activations layer by layer)\n",
    "        z1 = xb @ W1 + b1   # linear transformation: input to hidden layer\n",
    "        a1 = relu(z1)       # non linear activation on hidden outputs\n",
    "        z2 = a1 @ W2 + b2   # linear transform: hidden to output layer\n",
    "        a2 = softmax(z2)    # softmax converts outputs to probabilities \n",
    "\n",
    "        # comput loss (optional but helps monitor training)\n",
    "        if i % (batch_size * 100) == 0:\n",
    "            loss = cross_entropy(a2, yb)\n",
    "            print(f\"Epoch {epoch+1}, batch {i//batch_size}: loss = {loss:.4f}\")\n",
    "\n",
    "        # Backpropogation (manual derivative calculations for MLP)\n",
    "        dz2 = (a2 - yb) / batch_size    # derivative of loss wrt output logits\n",
    "        dW2 = a1.T @ dz2                # output weight gradients\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "        da1 = dz2 @ W2.T                # propogate gradient to hidden activations\n",
    "        dz1 = da1 * relu_deriv(z1)      # chain rule in ReLU derivative \n",
    "        dW1 = xb.T @ dz1                # input weight gradients\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "        # parameter update (gradient descent)\n",
    "        W2 -= lr * dW2  # output weights\n",
    "        b2 -= lr * db2  # output bias \n",
    "        W1 -= lr * dW1  # input weights\n",
    "        b1 -= lr * db1  # input bias \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8689310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.49%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model accuracy on test set (forward pass)\n",
    "z1 = X_test @ W1 + b1      # input to hidden\n",
    "a1 = relu(z1)              # ReLU activation \n",
    "z2 = a1 @ W2 + b2          # Hidden output\n",
    "a2 = softmax(z2)           # softmac for probabilities \n",
    "\n",
    "y_pred = np.argmax(a2, axis=1)  # predicted digits: class with maximum probability\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e405256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
